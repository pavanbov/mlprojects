{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566},{"sourceId":6140902,"sourceType":"datasetVersion","datasetId":3475059},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q transformers langchain peft bitsandbytes trl datasets notebook accelerate evaluate","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:42:02.249456Z","iopub.execute_input":"2024-04-14T23:42:02.249855Z","iopub.status.idle":"2024-04-14T23:42:18.634373Z","shell.execute_reply.started":"2024-04-14T23:42:02.249805Z","shell.execute_reply":"2024-04-14T23:42:18.633036Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from IPython.core.display import HTML, Markdown\ntable_css = \"\"\"\n    table {\n        align: left; display: block\n    }\n\"\"\"\nHTML('<style>{}</style>'.format(table_css))","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-14T23:42:18.637018Z","iopub.execute_input":"2024-04-14T23:42:18.637831Z","iopub.status.idle":"2024-04-14T23:42:18.646634Z","shell.execute_reply.started":"2024-04-14T23:42:18.637791Z","shell.execute_reply":"2024-04-14T23:42:18.645688Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table {\n        align: left; display: block\n    }\n</style>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Table of Contents\n1. [Introduction](#Introduction)<br>\n    1.1. [Aim of the project](#Aim-of-the-project)<br>\n2. [Setup and important aspects](#Setup-and-important-aspects)<br>\n    2.1. [Chat template](#Chat-template)<br>\n    2.2. [Prompt engineering](#Prompt-engineering)<br>\n    2.3. [Pipeline parameters](#Pipeline-parameters)<br>\n3. [Text summarization: Methods and strategies](#Text-summarization:-Methods-and-strategies)<br>\n    3.1. [Stuffing](#Stuffing)<br>\n    3.2. [MapReduce](#MapReduce)<br>\n    3.3. [Refine](#Refine)<br>\n    3.4. [Document splitting strategies](#Document-splitting-strategies)<br>\n4. [Experiments](#Experiments)<br>\n    4.1. [DIY: Run on your own write-up!](#DIY:-Run-on-your-own-write-up!)<br>\n5. [Fine-tuning Gemma with LoRa](#Fine-tuning-Gemma-with-LoRa)\n6. [Conclusions and next steps](#Conclusions-and-next-steps)\n\n# Introduction\n\nOver the years, the amount of data produced, copied and consumed in the world has grown exponentially, soaring from 2 Zettabytes in 2010 to projected estimates of [181 Zettabytes in 2025](https://www.statista.com/statistics/871513/worldwide-data-created/). <br>\nTo put these numbers into context, envision each byte as a grain of rice; one zettabyte (10^21 bytes) would be somewhat equivalent to filling [the Pacific Ocean with rice](https://www.oldcolony.us/wp-content/uploads/2014/11/whatisbigdata-DKB-v2.pdf).\n\nIn this context, one of the most common and useful tasks in the NLP field is **text summarization.** <br>\nSummarization is the process of extracting the **most important information from a text** and presenting it in a condensed form. Having quality condensed information can help individuals and organizations **reduce this information overload**, [optimizing processes while saving time and resources](https://nowigence.com/importance-benefits-of-auto-text-summarization/).\n\nThere are **mainly two** text summarization techniques: abstractive and extractive.\n\n- **Abstractive**: The text is summarized using the available context but using different words. This is a process that requires \"understanding\" the information contained in the text.\n- **Extractive**: The most relevant phrases or words are selected and extracted from the text, without rephrasing or generating new words.\n\nThe advent of LLM models (like Gemma) helped to **significantly improve** the [quality of the summaries produced](https://arxiv.org/pdf/2310.10449.pdf), so much that LLMs are now considered the [gold standard for text summarization](https://arxiv.org/pdf/2305.14239.pdf).\n\nHowever, there are **some considerations** to bear in mind when undertaking this task, in particular:\n\n1. **Quality**: although there are metrics like ROUGE to assess quality, a summary is often subjective and varies based on the target audience. Is the summary intended for a technical or non-technical audience? Should it provide detailed information or offer a high-level overview?\n2. **Hallucinations**: it is known that LLMs tend to have [hallucinations](https://arxiv.org/pdf/2401.11817.pdf), that is to generate plausible but incorrect information from a factual or logical point of view. This phenomenon can have significant impacts, especially when dealing with large documents where content is not known in advance, making it challenging to control.\n3. **Text length**: every LLM have a maximum context window and large documents can't fit entirely in one pass, thus different approaches are required.\n\n\n## Aim of the project\n\nIn this notebook, I will demonstrate the process of **text summarization using Gemma**, with a dedicated emphasis on **Kaggle writeups.** <br>\nThe key aspects I will discuss are:\n- Establishing a text summarization pipeline using Gemma and LangChain\n- Providing an overview of the crucial parameters and methods one should keep in mind while working with an LLM\n- Exploring summarization techniques, such as Stuffing, MapReduce and Refine\n- Fine-tuning Gemma using Parameter Efficient Fine-Tuning (PEFT)\n- Future considerations and next steps\n\nThis work aims to build a **comprehensive understanding of the task** and develop a pipeline that can serve as a **good starting point** for individuals interested in approaching summarization tasks using open-source models like Gemma on Kaggle.\n\n---\n\n# Setup and important aspects\n\nThis notebook will use the following building blocks:\n- **Gemma 2B**, in order to work even on commercial laptop with common GPUs\n- **HuggingFace**, which thanks to its abstraction levels allows you to work with LLM in a user-friendly way\n- **LangChain**, to build summarization pipelines even in the presence of large documents\n\nLet's start importing the model and pipeline that we will use with HuggingFace:","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom accelerate.utils import release_memory\nimport torch\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, PeftModel\nimport pandas as pd\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.text_splitter import CharacterTextSplitter, HTMLHeaderTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.docstore.document import Document\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nimport evaluate\nimport transformers\nfrom langchain.llms.base import LLM\nfrom typing import Any\nimport warnings\nimport gc\nimport random\nimport numpy as np\n\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Read writeups dataset\nwriteups = pd.read_csv('/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv')\nwriteups = writeups.drop_duplicates(subset=['link', 'writeup']).reset_index(drop=True)\n\n# Logging in HF\nhf_access_token = UserSecretsClient().get_secret(\"hf_token\")\nlogin(token = hf_access_token)\n\nmodel = \"/kaggle/input/gemma/transformers/2b-it/3\"\n\n# Load the HF pipeline using Gemma 2B from Kaggle\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    model_kwargs={\"torch_dtype\": torch.float16},\n    device='cuda',\n    max_new_tokens=512\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:42:18.648104Z","iopub.execute_input":"2024-04-14T23:42:18.648410Z","iopub.status.idle":"2024-04-14T23:42:33.707740Z","shell.execute_reply.started":"2024-04-14T23:42:18.648381Z","shell.execute_reply":"2024-04-14T23:42:33.706905Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-14 23:42:21.932731: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-14 23:42:21.932785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-14 23:42:21.934231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","output_type":"stream"},{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"name":"stdout","text":"Token is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"745286333d8d403a9dc2df0eee3ca20b"}},"metadata":{}}]},{"cell_type":"markdown","source":"Pipelines provide an efficient and user-friendly way to leverage models for inference. They consist of:\n- A tokenizer, which, if not explicitly specified, is automatically imported from the model configurations on HuggingFace\n- The model itself\n- Parameters for controlling and fine-tuning the output\n\nConsidering the code above, several crucial parameters have been configured:\n- `max_new_tokens` - controls the **maximum number of newly generated tokens**. If not specified, the default value may not be sufficient to generate enough text (therefore summaries).\n- `model_kwargs` - here we control the **precision** using `torch.float16`, with beneficial effects on [memory](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#:~:text=Mixed%20precision%20training%20achieves%20all,bit%20floating%20point%20everywhere%20else.).\n\nNow let's test our pipeline on a the first writeup from our dataset, taken from [here](https://www.kaggle.com/c/asl-signs/discussion/406306). ","metadata":{}},{"cell_type":"code","source":"# Import the first writeup from the dataset and render the first 1000 chars\nwriteup = writeups.iloc[0, 9]\nprint('Number of characters:', len(writeup))\nprint(writeup[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:42:33.709088Z","iopub.execute_input":"2024-04-14T23:42:33.709753Z","iopub.status.idle":"2024-04-14T23:42:33.715988Z","shell.execute_reply.started":"2024-04-14T23:42:33.709717Z","shell.execute_reply":"2024-04-14T23:42:33.715077Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of characters: 9864\n<h2>TLDR</h2>\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\n<p>We used only competition data.</p>\n<h2>1. Data Preprocessing</h2>\n<h3>1.1 CNN Preprocessing</h3>\n<ul>\n<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\n<li>During training, we applied various augmentations.</li>\n<li>We implemented standard normalization.</li>\n<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\n<li>We interpolated the time axis to a siz\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\\n\\n{}\".format(writeup)\n    }\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\n\ndisplay(Markdown(outputs[0][\"generated_text\"][len(prompt):].replace('#', ''))) # Replace possible headings to avoid conflict with Kaggle ToC","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:42:33.719308Z","iopub.execute_input":"2024-04-14T23:42:33.719719Z","iopub.status.idle":"2024-04-14T23:43:05.830962Z","shell.execute_reply.started":"2024-04-14T23:42:33.719689Z","shell.execute_reply":"2024-04-14T23:43:05.829985Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Chapter 1: Introduction**\n\n* Overview of the project: using an EfficientNet-B0 model for lip and pose classification.\n* Data preparation:\n    * 18 lip points, 20 pose points, and all hand points were extracted.\n    * Various augmentations and transformer pre-processing were applied.\n    * The input size was 160x80x3.\n\n**Chapter 2: Data Preprocessing**\n\n* CNN pre-processing:\n    * Global affine, shift-scale-rotate, and flip pose were applied.\n    * Mixup augmentation was used for CNNs.\n* Transformer pre-processing:\n    * Only 61 points were kept, including 40 lip points and 21 hand points.\n    * Randomly selected distances and angles were included.\n\n**Chapter 3: Training**\n\n* CNN training:\n    * One-fold cross-validation with a random split and 0.1 warm-up.\n    * Weighted cross-entropy loss with class weights.\n    * EfficientNet-B0 with 5 blocks and 256 hidden units.\n* Transformer training:\n    * One-fold cross-validation with a random split and 0.1 warm-up.\n    * Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.\n    * 4-layer, 256 hidden-size, 512 intermediate-size transformer.\n\n**Chapter 4: Hyperparameter Tuning**\n\n* Optuna was used to tune most parameters.\n* The parameters list for CNN and transformer training are provided.\n\n**Chapter 5: Submissions and Ensemble**\n\n* EfficientNet-B0 achieved a leaderboard score of approximately 0.8.\n* Ensemble of EfficientNet-B0, BERT, and DeBERTa was created.\n* A key feature was using the ensemble without softmax, which provided a boost of around 0.01.\n\n**Chapter 6: Conclusion**\n\n* The project achieved a high accuracy on the lip and pose classification task.\n* The EfficientNet-B0 model with ensemble achieved the best performance.\n* The conversion of DepthwiseConv2D operation was a challenge, but a faster version was developed."},"metadata":{}}]},{"cell_type":"markdown","source":"The output looks great, and this was possible thanks to **key features that we will now explore.**\n\n## Chat template\nThe main use of LLMs is in a **chat style setup**. This means that instead of continuing a text string, the model receives messages in which **\"roles\" are present.** <br>\nJust as there are different tokenizers for different models, each model expects different chat templates.\n\n[Gemma's technical documentation](https://ai.google.dev/gemma/docs/formatting) outlines the necessity of employing specific tokens to indicate roles:\n- Token to indicate a user turn: `user`\n- Token to indicate a model turn: `model`\n- Token to indicate the beginning of dialogue turn: `<start_of_turn>`\n- Token to indicate the end of dialogue turn: `<end_of_turn>`\n\nIn the code above, we achieved that thanks to the presence of the following piece of code:\n\n```\nmessages = [\n    {\"role\": \"user\",\n     \"content\": \"....\"}\n     ]\n\nprompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n```\nTo better understand what is happening, let's try a test chat:","metadata":{}},{"cell_type":"code","source":"test_messages = [\n    {\"role\": \"user\",\n     \"content\": \"This is a test\"},\n    {\"role\": \"assistant\",\n     \"content\": \"Good for you!\"},\n    {\"role\": \"user\",\n     \"content\": \"Ah ah\"},\n]\n\ntest_prompt = pipe.tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\nprint(test_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:05.832311Z","iopub.execute_input":"2024-04-14T23:43:05.833109Z","iopub.status.idle":"2024-04-14T23:43:05.839140Z","shell.execute_reply.started":"2024-04-14T23:43:05.833068Z","shell.execute_reply":"2024-04-14T23:43:05.838211Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nThis is a test<end_of_turn>\n<start_of_turn>model\nGood for you!<end_of_turn>\n<start_of_turn>user\nAh ah<end_of_turn>\n<start_of_turn>model\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Given that LLMs generate text predicting the next token, [HuggingFace](https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts) provides the `apply_chat_template` function to **ensure the model generates text as a response to an input** rather than as a continuation of the user's prompt. This is achieved using the `add_generation_prompt` parameter, which adds the `<start_of_turn>` token, **reducing the possibility of generating text that continues the user's message.**\n\n## Prompt engineering\n\nObtaining quality output also depends on the prompt structure. <br>\nThe query used was:\n```\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary into chapters, be impersonal and use bullet points:\n\n[writeup]\n```\nDepending on the audience, this **prompt may vary** to allow us to generate summaries more **aligned with our target.** <br>\nFor example, let's assume we need to create a writeup summary to a less technical audience:","metadata":{}},{"cell_type":"code","source":"messages_eli5 = [\n    {\"role\": \"user\",\n     \"content\": \"Summarize the following text and explain it like I'm 5 years old:\\n\\n{}\".format(writeup)},\n]\n\nprompt_eli5 = pipe.tokenizer.apply_chat_template(messages_eli5, tokenize=False, add_generation_prompt=True)\noutputs_eli5 = pipe(\n    prompt_eli5,\n    add_special_tokens=True,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3\n)\n\ndisplay(Markdown(outputs_eli5[0][\"generated_text\"][len(prompt_eli5):].replace('#', ''))) # Replace possible headings to avoid conflict with Kaggle ToC","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:05.840277Z","iopub.execute_input":"2024-04-14T23:43:05.840549Z","iopub.status.idle":"2024-04-14T23:43:24.762936Z","shell.execute_reply.started":"2024-04-14T23:43:05.840527Z","shell.execute_reply":"2024-04-14T23:43:24.761893Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Sure, here's a summary of the text:\n\n**Introduction:**\n\n* The task is to classify lip and pose landmarks in images.\n* Different models are trained and combined to achieve high accuracy.\n\n**Data Preprocessing:**\n\n* 80 facial landmarks are extracted from the input image.\n* The data is augmented with various techniques to increase the size and diversity of the training data.\n\n**Model Training:**\n\n* EfficientNet-B0 is used as the base model.\n* Different augmentation and pre-training techniques are applied to improve the model'sgeneralizability.\n* Transformers are also trained and combined with EfficientNet-B0 for better performance.\n\n**Evaluation and Ensemble:**\n\n* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n* The ensemble achieves a leaderboard score of 0.81, demonstrating its effectiveness.\n\n**Key Points:**\n\n* Depthwise convolution outperforms other models, especially for this task.\n* The ensemble approach significantly improves the model'sgeneralizability.\n* The EfficientNet-B0 model is very efficient and achieves high accuracy.\n\n**Additional Notes:**\n\n* The text provides detailed descriptions of the data pre-processing, model training, and evaluation steps.\n* It also provides insights into the challenges and techniques used to achieve high performance."},"metadata":{}}]},{"cell_type":"markdown","source":"The summary is overall easier to understand thanks to the addition of `explain it like I am a 5 years old`, also known as [eli5](https://www.dictionary.com/e/slang/eli5/).\n\nIt's worth mentioning that there are **other techniques** that can help achieve better results, one of this is called called [Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot). <br>\nFew shot prompring involves utilizing **examples as conditioning for subsequent examples, guiding the model in generating the desired responses.**\n\nLet's demonstrate few shot conditioning using the pipeline we developed so far:","metadata":{}},{"cell_type":"code","source":"messages_few_shot = [\n    {\"role\": \"user\",\n     \"content\": \"This film was great, rich of details and with great actors.\"},\n    {\"role\": \"assistant\",\n     \"content\": \"SENTIMENT: Positive.\\nSUBJECT: Film\"},\n    {\"role\": \"user\",\n     \"content\": \"This park is dirty.\"},\n    {\"role\": \"assistant\",\n     \"content\": \"SENTIMENT: Negative.\\nSUBJECT: Park\"},\n    {\"role\": \"user\",\n     \"content\": \"This notebook is fantastic. I'm learning a lot\"},\n]\n\nprompt_few_shot = pipe.tokenizer.apply_chat_template(messages_few_shot, tokenize=False, add_generation_prompt=True)\noutputs_few_shot = pipe(\n    prompt_few_shot,\n    add_special_tokens=True,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3\n)\nprint(outputs_few_shot[0][\"generated_text\"][len(prompt_few_shot):])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:24.764239Z","iopub.execute_input":"2024-04-14T23:43:24.764614Z","iopub.status.idle":"2024-04-14T23:43:25.405973Z","shell.execute_reply.started":"2024-04-14T23:43:24.764582Z","shell.execute_reply":"2024-04-14T23:43:25.404992Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"SENTIMENT: Positive.\nSUBJECT: Notebook\nThe book is well-written and engaging.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model learned how to perform the task **with just few examples using the `apply_chat_template`.** Without any request, it correctly understood that we want two lines, one with `SENTIMENT` and one with the `SUBJECT` of the sentence.\n\nWhile applying Few-Shot Learning could be viable for our scenario, **it would require examples of real summarization of Kaggle writeups passed as examples**, potentially resulting in **overly large prompte**. Considering that the length of a text can be problematic (more on that later), **we will stick to a simpler approach** by working on our prompt engineering skills like we did before, which has already yielded good results.","metadata":{}},{"cell_type":"markdown","source":"## Pipeline parameters\n\nYou noticed that the pipeline uses few paramenters that are essential for controlling our output. <br>\nHere's the list along with some rationale:\n\n- `do_sample`: this parameter enables **decoding strategies** to select the next token from the probability distribution over the entire vocabulary. Together with `num_beams`, we can control [different strategies](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin). I opted for `True` and `num_beams=1` (default), which is the multinomial sampling. More of decoding strategies [here](https://deci.ai/blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/) and [here](https://huggingface.co/docs/transformers/main/en/generation_strategies#decoding-strategies).\n- `temperature`: this parameter controls the **randomness**. The lower the temperature, the more deterministic the results are in the sense that the highest probable token is picked. I opted for a very low value because we need to encourage **more factual responses** and not creative ones.\n- `top_p`: according to the [documentation](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.top_p), this parameter controls the **sampling of tokens**. The higher the value, the higher the chances that the model will look to more possible words, including less likely ones. Again, I opted for a relatively low value **to maintain coherence** given the task of summarization.\n- `top_k`: in simple terms, together with `top_p`, [it controls the number of tokens](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.top_k) to keep for prediction. Once again, a low value will favour **less creative responses**, which is exactly what we are looking for in this notebook.\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br> <br>\n    - Following a <b>chat template</b> is highly recommented in order to <b>mimick model's training process</b> (therefore, model's knowledge).<br>\n    - <b>Prompt engineering</b> is mandatory: a poor prompt will lead to poor results. Techniques such as <b>Few-Shot learning</b> can be useful tools in our arsenale. <br>\n    - Controlling the generation <b>parameters</b> is important and depends on the task, whether <b>we seek creativity or factual responses.</b>\n</div>\n\nNow it's time to talk about some **important aspects** of text summarization and **LangChain!**\n\n---\n\n# Text summarization: Methods and strategies\n\nThere are many techniques and ways to perform a text summarization, but I will focus only on **three strategies: Stuffing, MapReduce and Refine.** These ideas where based on this amazing blog post you can find [here](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed).\nThe methods are simple and easy to understand, yet quite powerful.\n\n## Stuffing\n\n<img src=\"https://i.imgur.com/UXm5AvD.jpeg\" width=\"600\">\n\nStuffing is pretty straightforward: we **pass the entire data** to the LLM by stuffing it into the prompt as context. **This is exactly what we did so far using Gemma.**\n\n| Pros ✅| Cons ❌|\n|---|---|\n| Only a single call to the LLM | The data can surpass the model's context length, thus this method could not be feasible <br> for larger text files |\n| Comprehensive context, since the model have access to the <br> entire information | The quality might not be ideal for extensive  documents, <br> as some information might be skipped |\n\n\n## MapReduce\n\n<img src=\"https://i.imgur.com/q4i1pR7.jpeg\" width=\"600\">\n\nMapReduce introduces a **multi-stage summarization**:\n- First we split the document into chunks\n- We perform text summarization for each chunk\n- One final call to the LLM is used to create a comprehensive final summary, using all the summaries as input\n\n| Pros ✅ | Cons ❌|\n|---|---|\n| The context limit is no longer a problem, since the document <br> is broken in manageable chunks | Multiple LLM calls are required, affecting processing time |\n| Each chunk can be process in parallel, thus speeding <br> the summarization task | Potential loss of information due to the fact that the LLM only sees each chunk <br> indipendently without a context |\n\n\n## Refine\n\n<img src=\"https://i.imgur.com/7VN4IpZ.jpeg\" width=\"600\">\n\nThe last method is called Refine, and follows an **iterative approach**:\n- The document is split into chunks, just like the MapReduce\n- The first chunk is summarized\n- For each following chunk, the previous output (summary) is combined with the new information\n- The LLM is instructed to improve (refine!) the previous summary\n\n| Pros ✅| Cons ❌|\n|---|---|\n| It solves MapReduce's potential loss of information while retaining <br> the ability to process very large files| Multiple LLM calls are required affecting processing time |\n| It follows a sequentiality, thus potentially improve summary quality | LLM errors and hallucinations can propagade during each iteration, <br>thus affecting the final quality |\n\n\n## Document splitting strategies\n\nMapReduce and Refine depend heavily on document splitting. While the concept of splitting is easy to understand, **this step is often intricate.**\n\nFirst of all, when we deal with LLM we deal with **tokens**. <br>\nTokens are [pieces of words](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them): when we write a prompt, the input is transformed into tokens. One token doens't mean one word, but we can generally approximate **1 token ~ 4 characters in English ~ 3/4 of a word**. Simply put, 75 words ~ 100 tokens. <br>\nDepending on the model used, we can accomodate a certain amount of tokens **shared between prompt and model's generation**, thus forcing us to operate some **splitting if the context is too large**. [Gemma](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf) has a **maximum context length of 8192 tokens**, which roughly translates to more than **6100 words.**\n\nDocument splitting can follow different strategies. For example, it can be based on a **character** (`'\\n'` or `'\\n\\n'` which often delimit new sentences or paragraphs) or on the **document structure** (such as chapters or sections etc). <br>\nThe choice of the splitting strategy depends on the task and document we need to analyze. For our task, it is reasonable assume the following scenarios:\n- **Scenario A - Stuffing is feasible**: This is the best case scenario, we can simply pass the entire context to the model.\n- **Scenario B - Stuffing is feasible but output is poor**: Given that it's unlikely that a writeup exceeds 6100 words, this scenario could arise if the quality of the Stuffing summarization is poor. Our model might skip some useful information if provided with the entire text, therefore a possible approach could be **splitting writeups based on sections**. The Kaggle template works well because is divided into clear sections, allowing for a smooth split while keeping semantic sentences in the same split.\n- **Scenario C - Output is poor and writeups don't follow a clear structure**: This is the most difficult, yet plausible, scenario, in which our model struggles with the winner's stream of consciousness and lack of a clear document structure. Moreover, if the writeup is lengthy, the situation could be particularly challenging. In such cases, a **character splitting strategy could be ideal.**\n\nLet's see it in action, testing a fake writeup formatted as per the [documentation](https://www.kaggle.com/solution-write-up-documentation) and a messy one.","metadata":{}},{"cell_type":"code","source":"example_clean_writeup = \"\"\"\n# Context section\nThis section only contains 2 links \n# Data context\nlink to the competition data page\n# Overview of the Approach\nthis section should describe the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\n# Details of the submission\nthis section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work.\n# Sources\nthis section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\"\n\nexample_messy_writeup = \"\"\"\nThis section only contains 2 links, and here the link to the competition data page.\nPartial section.\nAnother partial section.\nExtensive model secondi which describes the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\nSpecial section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work. Last section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.407217Z","iopub.execute_input":"2024-04-14T23:43:25.407555Z","iopub.status.idle":"2024-04-14T23:43:25.412951Z","shell.execute_reply.started":"2024-04-14T23:43:25.407529Z","shell.execute_reply":"2024-04-14T23:43:25.412056Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We'll now use LangChain [text splitters methods](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to demonstrate document splitting. [LangChain](https://python.langchain.com/docs/get_started/introduction) is a framework for developing applications powered by language models and it's great because **simplify tasks** for developing LLM-powered applications, which will easily let us experiment with the above techniques **without re-inventing the wheel.**","metadata":{}},{"cell_type":"code","source":"# Split the clean writeup based on sections\ntext_splitter = CharacterTextSplitter(separator='#', chunk_size=100, chunk_overlap=10)\ntexts_clean_writeup = text_splitter.split_text(example_clean_writeup)\n\n# Print the first characters of each split\nprint([i[:50] for i in texts_clean_writeup])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.414249Z","iopub.execute_input":"2024-04-14T23:43:25.414549Z","iopub.status.idle":"2024-04-14T23:43:25.429202Z","shell.execute_reply.started":"2024-04-14T23:43:25.414521Z","shell.execute_reply":"2024-04-14T23:43:25.428326Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['# Context section\\nThis section only contains 2 lin', 'Data context\\nlink to the competition data page', 'Overview of the Approach\\nthis section should descr', 'Details of the submission\\nthis section should incl', 'Sources\\nthis section should include links to helpf']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Some important parameters to consider:\n- **separator**: the character to split the text on\n- **chunk_size**: number of characters in each chunk\n- **chunk_overlap**: if we want to overlap the current chunk with previous text\n\nWhat `CharacterTextSplitter` does in this example is it first looks for the **first 100 characters** and then **splits the next chunk** from the closest separator.\n\nIn case of a messy writeup, we could opt for a sentence separation:","metadata":{}},{"cell_type":"code","source":"# Split the messy writeup based on newlines\ntext_splitter = CharacterTextSplitter(separator='\\n', chunk_size=100, chunk_overlap=50)\ntexts_messy_writeup = text_splitter.split_text(example_messy_writeup)\n\nprint([i[:50] for i in texts_messy_writeup])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.430299Z","iopub.execute_input":"2024-04-14T23:43:25.430632Z","iopub.status.idle":"2024-04-14T23:43:25.440978Z","shell.execute_reply.started":"2024-04-14T23:43:25.430600Z","shell.execute_reply":"2024-04-14T23:43:25.440046Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['This section only contains 2 links, and here the l', 'Partial section.\\nAnother partial section.', 'Extensive model secondi which describes the models', 'Special section should include what was special, c']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this case, we can see also the effect of the `chunk_overlap`, as the second document includes a `\\n` in its body. This is because we **told the function to overlap** chunks by 50 characters.\n\nThese examples are simplified; actual writeups tend to be more intricate, but **we can leverage HTML formatting**. LangChain comes with `HTMLHeaderTextSplitter`, let's test it using the previous writeup:","metadata":{}},{"cell_type":"code","source":"# Split on HTML headers\nheaders_to_split_on = [\n    (\"h1\", \"Header 1\"),\n    (\"h2\", \"Header 2\")\n]\n\n# Split the real HTML writeup based on headers\ntext_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=False)\ntexts_html_writeup = text_splitter.split_text(writeup)\n\nprint('Length writup:', len(writeup))\nprint('Number of splits:', len(texts_html_writeup))\nprint('Element returned:', type(texts_html_writeup[0]))\nprint('Length of each split:', [len(i.page_content) for i in texts_html_writeup])\n\n# Print the first characters for each split\nprint(); print([(i.page_content[:50], i.metadata) for i in texts_html_writeup])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.442082Z","iopub.execute_input":"2024-04-14T23:43:25.442408Z","iopub.status.idle":"2024-04-14T23:43:25.480987Z","shell.execute_reply.started":"2024-04-14T23:43:25.442385Z","shell.execute_reply":"2024-04-14T23:43:25.480113Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Length writup: 9864\nNumber of splits: 6\nElement returned: <class 'langchain_core.documents.base.Document'>\nLength of each split: [511, 1567, 1040, 1203, 1031, 1260]\n\n[('We used an approach similar to audio spectrogram c', {'Header 2': 'TLDR'}), ('We extracted 18 lip points, 20 pose points (includ', {'Header 2': '1. Data Preprocessing'}), ('These augmentations are used in both CNN training ', {'Header 2': '2. Augmentation'}), ('Train on one fold with a random split (8 folds in ', {'Header 2': '3. Training'}), ('We rewrote all our models in Keras and transferred', {'Header 2': '4. Submissions, Conversion and Ensemble'}), ('Depthwise convolution models performed very well f', {'Header 2': '5. PS. Need BETTER TFlite DepthwiseConv2D'})]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's clarify we we got:\n- The returned structure contains **6 splits** based on HTML headers (h1 and h2, but we can also specify more)\n- Each split is made of a [Document class](https://api.python.langchain.com/en/v0.0.339/schema/langchain.schema.document.Document.html), a specific structure to store text and metadata. As a matter of fact, each Document is made of the **page content and the headers (as metadata).**\n    \nWe could **add the metadata information** back in the page content so that our model will have **access to the section titles for better context.**","metadata":{}},{"cell_type":"code","source":"for i, text in enumerate(texts_html_writeup):\n    # Join the metadata and the content together\n    final_content = '\\n'.join(text.metadata.values()) + '\\n' + text.page_content\n    # Replace the old content with the enriched one\n    text.page_content = final_content\n    \n    # Print some examples\n    if i < 2:\n        print(final_content); print()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.482102Z","iopub.execute_input":"2024-04-14T23:43:25.482862Z","iopub.status.idle":"2024-04-14T23:43:25.490133Z","shell.execute_reply.started":"2024-04-14T23:43:25.482825Z","shell.execute_reply":"2024-04-14T23:43:25.488809Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"TLDR\nWe used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \nWe used only competition data.\n\n1. Data Preprocessing\nWe extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \nOnly 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \nAugmentations, normalization and NaN-filling were applied sequentially.  \nSequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \nApart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \nMotion features consist of future motion and history motion, which can be denoted as:  \n$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \nFull 210 pairwise distances among 21 hand points were included.  \nThere are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \nRandomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can could use `CharacterTextSplitter` again, this time using `split_documents()` if we want to further apply additional **splitting within each new chunk** with a given `chunk_size`. <br> This could be helpful, for instance, for **very large documents** when even after splitting into sections doesn't separate the text enough. ","metadata":{}},{"cell_type":"code","source":"text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n\n# Split\nsplits = text_splitter.split_documents(texts_html_writeup)\nprint('Number of final splits:', len(splits))\nprint('Length of each final split:', [len(i.page_content) for i in splits])\n\nprint(); print([(i.page_content[:50], i.metadata) for i in splits])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.497274Z","iopub.execute_input":"2024-04-14T23:43:25.497655Z","iopub.status.idle":"2024-04-14T23:43:25.506321Z","shell.execute_reply.started":"2024-04-14T23:43:25.497615Z","shell.execute_reply":"2024-04-14T23:43:25.504842Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Number of final splits: 6\nLength of each final split: [516, 1589, 1056, 1215, 1071, 1302]\n\n[('TLDR\\nWe used an approach similar to audio spectrog', {'Header 2': 'TLDR'}), ('1. Data Preprocessing\\nWe extracted 18 lip points, ', {'Header 2': '1. Data Preprocessing'}), ('2. Augmentation\\nThese augmentations are used in bo', {'Header 2': '2. Augmentation'}), ('3. Training\\nTrain on one fold with a random split ', {'Header 2': '3. Training'}), ('4. Submissions, Conversion and Ensemble\\nWe rewrote', {'Header 2': '4. Submissions, Conversion and Ensemble'}), ('5. PS. Need BETTER TFlite DepthwiseConv2D\\nDepthwis', {'Header 2': '5. PS. Need BETTER TFlite DepthwiseConv2D'})]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Given that we selected a character window larger than the document length, we got the same result as before. The length of each split is a **little longer** because in the previous step **we added the metadata to the text.**<br>\n\nNow that everything is clear, let's run some experiment! We'll use the `splits` variable in a moment with MapReduce and Refine!\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br> <br>\n    - <b>Stuffing, MapReduce and Refine</b> are three different techniques that can be used to summarize documents. <br>\n    - Given the task, document structure and model capabilities, we might need to <b>split our document in chunks</b> to <b>fit the context</b> in our prompt or to improve the summary.<br>\n    - Kaggle writeups can all potentially fit in <b>Gemma given its context length</b>, but different strategies such as <b>Sections splitting</b> based on HTML formatting could potentially be tested.\n</div>\n\n---\n\n# Experiments\n\nTo setup our experiments, we'll first wrap our HuggingFace pipeline in LangChain [following this guide](https://python.langchain.com/docs/modules/model_io/llms/custom_llm):","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\ngc.collect()\n\nclass GemmaLLM(LLM):\n    hf_pipe: Any = None\n    pipe_kwargs: Any = None\n        \n    def __init__(self, hf_pipeline, pipe_kwargs):\n        super(GemmaLLM, self).__init__()\n        self.hf_pipe = hf_pipeline\n        self.pipe_kwargs = pipe_kwargs\n\n    @property\n    def _llm_type(self):\n        return \"Gemma pipeline\"\n\n    def _call(self, prompt, **kwargs):\n        \"\"\"\n        This is the part that gets invoked by LangChain. We make sure that we pass the parameters we\n        previously discussed to the HF pipeline, returning only the output without the prompt.\n        \"\"\"\n        outputs = self.hf_pipe(\n            prompt,\n            do_sample=self.pipe_kwargs['do_sample'],\n            temperature=self.pipe_kwargs['temperature'],\n            top_k=self.pipe_kwargs['top_k'],\n            top_p=self.pipe_kwargs['top_p'],\n            add_special_tokens=self.pipe_kwargs['add_special_tokens']\n        )\n        return outputs[0][\"generated_text\"][len(prompt):]  \n\n    @property\n    def _identifying_params(self):\n        \"\"\"Pipeline params\"\"\"\n        return {\"n\": self.pipe_kwargs}\n\nlangchain_hf = GemmaLLM(hf_pipeline=pipe,\n                        pipe_kwargs={\n                            'do_sample':True,\n                            'temperature':0.1,\n                            'top_k':20,\n                            'top_p':0.3,\n                            'add_special_tokens':True\n                })","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.507792Z","iopub.execute_input":"2024-04-14T23:43:25.508119Z","iopub.status.idle":"2024-04-14T23:43:25.885199Z","shell.execute_reply.started":"2024-04-14T23:43:25.508085Z","shell.execute_reply":"2024-04-14T23:43:25.884177Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"To see it in action, let's test it on the prompt we created at the beginning:","metadata":{}},{"cell_type":"code","source":"print(prompt[:350])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.886912Z","iopub.execute_input":"2024-04-14T23:43:25.887259Z","iopub.status.idle":"2024-04-14T23:43:25.902591Z","shell.execute_reply.started":"2024-04-14T23:43:25.887233Z","shell.execute_reply":"2024-04-14T23:43:25.900805Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n<h2>TLDR</h2>\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer mod\n","output_type":"stream"}]},{"cell_type":"code","source":"out = langchain_hf.invoke(prompt)\ndisplay(Markdown(out.replace('#', ''))) # Replace possible headings to avoid conflict with Kaggle ToC","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:43:25.903896Z","iopub.execute_input":"2024-04-14T23:43:25.904342Z","iopub.status.idle":"2024-04-14T23:43:57.750296Z","shell.execute_reply.started":"2024-04-14T23:43:25.904309Z","shell.execute_reply":"2024-04-14T23:43:57.749244Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Chapter 1: Introduction**\n\n* Overview of the project: using an EfficientNet-B0 model for lip and pose classification.\n* Data preparation:\n    * 18 lip points, 20 pose points, and all hand points were extracted.\n    * Various augmentations and transformer pre-processing were applied.\n    * The input size was 160x80x3.\n\n**Chapter 2: Data Preprocessing**\n\n* CNN pre-processing:\n    * Global affine, shift-scale-rotate, and flip pose were applied.\n    * Mixup augmentation was used for CNNs.\n* Transformer pre-processing:\n    * Only 61 points were kept, including 40 lip points and 21 hand points.\n    * Randomly selected distances and angles were included.\n\n**Chapter 3: Training**\n\n* CNN training:\n    * One-fold cross-validation with a random split and 0.1 warm-up.\n    * Weighted cross-entropy loss with class weights.\n    * EfficientNet-B0 with 5 blocks and 256 hidden units.\n* Transformer training:\n    * One-fold cross-validation with a random split and 0.1 warm-up.\n    * Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.\n    * 4-layer, 256 hidden-size, 512 intermediate-size transformer.\n\n**Chapter 4: Hyperparameter Tuning**\n\n* Optuna was used to tune most parameters.\n* The parameters list for CNN and transformer training are provided.\n\n**Chapter 5: Submissions and Ensemble**\n\n* EfficientNet-B0 achieved a leaderboard score of approximately 0.8.\n* Ensemble of EfficientNet-B0, BERT, and DeBERTa was created.\n* A key feature was using the ensemble without softmax, which provided a boost of around 0.01.\n\n**Chapter 6: Conclusion**\n\n* The project achieved a high accuracy on the lip and pose classification task.\n* The EfficientNet-B0 model with ensemble achieved the best performance.\n* The conversion of DepthwiseConv2D operation was a challenge, but a faster version was developed."},"metadata":{}}]},{"cell_type":"markdown","source":"Everything is still working as expected: we wrapped our pipeline and passed the same parameters. Once again, **this was an example of the Stuffing method.**\n\nLet's see how MapReduce and Refine methods perform using the split based on HTML tags we created before.","metadata":{}},{"cell_type":"code","source":"# MapReduce strategy\n\n# Define prompt for summarization of each chunk\nprompt_template = \"\"\"<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n{text}<end_of_turn>\n<start_of_turn>model\"\"\"\nprompt_init = PromptTemplate.from_template(prompt_template)\n\n# Define prompt for final output, the summary of summaries\ncombine_template = \"\"\"<bos><start_of_turn>user\nYou are given a text containing summaries of different part of a document.\nCreate one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n\n{text}<end_of_turn>\n<start_of_turn>model\"\"\"\ncombine_prompt = PromptTemplate.from_template(combine_template)\n\n# Create the chain of summarization, using map_reduce\nchain = load_summarize_chain(langchain_hf, chain_type='map_reduce', map_prompt=prompt_init, combine_prompt=combine_prompt)\n\n# Run the chain on the chunks\nout_summary = chain.invoke(splits)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:43:57.751759Z","iopub.execute_input":"2024-04-14T23:43:57.752141Z","iopub.status.idle":"2024-04-14T23:45:04.157567Z","shell.execute_reply.started":"2024-04-14T23:43:57.752106Z","shell.execute_reply":"2024-04-14T23:45:04.156712Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1610 > 1024). Running this sequence through the model will result in indexing errors\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_1138/1740229327.py\", line 24, in <module>\n    out_summary = chain.invoke(splits)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\n    self._call(inputs, run_manager=run_manager)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\n    output, extra_return_dict = self.combine_docs(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py\", line 237, in combine_docs\n    result, extra_return_dict = self.reduce_documents_chain.combine_docs(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/reduce.py\", line 246, in combine_docs\n    return self.combine_documents_chain.combine_docs(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\n    return self(kwargs, callbacks=callbacks)[self.output_key]\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n    return wrapped(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\n    return self.invoke(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\n    self._call(inputs, run_manager=run_manager)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\n    response = self.generate([inputs], run_manager=run_manager)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\n    return self.llm.generate_prompt(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 597, in generate_prompt\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 767, in generate\n    output = self._generate_helper(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 621, in _generate_helper\n    self._generate(\n  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1233, in _generate\n    else self._call(prompt, stop=stop, **kwargs)\n  File \"/tmp/ipykernel_1138/2361379547.py\", line 23, in _call\n    outputs = self.hf_pipe(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\n    return super().__call__(text_inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n","output_type":"stream"}]},{"cell_type":"code","source":"display(Markdown(out_summary['output_text'].replace('#', '')))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:45:04.158668Z","iopub.execute_input":"2024-04-14T23:45:04.158941Z","iopub.status.idle":"2024-04-14T23:45:04.165317Z","shell.execute_reply.started":"2024-04-14T23:45:04.158918Z","shell.execute_reply":"2024-04-14T23:45:04.164409Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Chapter 1: Introduction**\n\n- The task is to classify images into different categories.\n- We use an approach similar to audio spectrogram classification.\n- We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n\n**Chapter 5: Data Preprocessing**\n\n- Extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points.\n- Applied various augmentations to the data.\n- Implemented standard normalization.\n- Filled in NaN values with zeros.\n- Interpolated the time axis to a size of 160 using 'nearest' interpolation.\n\n**Chapter 6: Feature Extraction**\n\n- Only 61 points were kept, including 40 lip points and 21 hand points.\n- For left and right hand, the one with less NaN was kept.\n- If right hand was kept, mirror it to left hand.\n\n**Chapter 7: Feature Engineering**\n\n- Hand-crafted features were also used, including motion, distances, and cosine of angles.\n- Motion features consist of future motion and history motion.\n- Full 210 pairwise distances among 21 hand points were included.\n- 15 angles of 5 fingers were included.\n\n**Chapter 8: Data Augmentation**\n\n- Sequences longer than 96 were interpolated to 96.\n- Sequences shorter than 96 were unchanged."},"metadata":{}}]},{"cell_type":"markdown","source":"Considering that we divided our writeup in chunks, the results is still decent. We lost some coherence as the model can't access the entire document at once. <br>\nBy setting `verbose=true`, we can also see the steps of the chain (**expand the output cell!**):","metadata":{}},{"cell_type":"code","source":"# Repeat the process above, with verbose True\nchain = load_summarize_chain(langchain_hf, chain_type='map_reduce', verbose=True, map_prompt=prompt_init, combine_prompt=combine_prompt)\n\n# Run the chain on the chunks\nout_summary = chain.invoke(splits)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:45:04.166536Z","iopub.execute_input":"2024-04-14T23:45:04.166854Z","iopub.status.idle":"2024-04-14T23:46:09.862203Z","shell.execute_reply.started":"2024-04-14T23:45:04.166819Z","shell.execute_reply":"2024-04-14T23:46:09.861211Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\nTLDR\nWe used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \nWe used only competition data.<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n1. Data Preprocessing\nWe extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \nOnly 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \nAugmentations, normalization and NaN-filling were applied sequentially.  \nSequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \nApart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \nMotion features consist of future motion and history motion, which can be denoted as:  \n$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \nFull 210 pairwise distances among 21 hand points were included.  \nThere are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \nRandomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n2. Augmentation\nThese augmentations are used in both CNN training and transformer training  \nRandom affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).  \nRandom interpolation: Slightly scale and shift the time dimension.  \nFlip pose: Flip the x-coordinates of all points. In CNN, x_new = x_max - x_old. In transformer, x_new = 2 * frame[:,0,0] - x_old.  \nFinger tree rotate: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb ([0,1,2,3,4]), these 4 root-children pairs are: 0-[1,2,3,4],1-[2,3,4],2-[3,4],3-[4]. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.  \nMixup: Implement basic mixup augmentation (only works with CNNs, not transformers). Replace augmentation: Replace some random parts from other samples of the same class. Time and frequence masking: This basic torchaudio augmentation works exceptionally well.  \nBefore augmentation:  \nAfter augmentation:<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n3. Training\nTrain on one fold with a random split (8 folds in total) or the full dataset using the best parameters Onecycle scheduler with 0.1 warmup. Use weighted CrossEntropyLoss. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat) Implement a hypercolumn for EfficientNet with 5 blocks  \nTrain on one fold with a random split (8 folds in total) or the full dataset using the best parameters Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule. A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained. A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.  \nSince we trained only one fold and used smaller models, we decided to tune most parameters with Optuna.  \nHere is the parameters list of CNN training (transformer training has a similar param-list):  \nAll augmentations probabilities (0.1 - 0.5+)  \nLearning rate (2e-3 - 3e-3)  \nDrop out (0.1 - 0.25)  \nNum of epochs (170-185)  \nLoss weights powers (0.75 - 2)  \nOptimizer (Lookahead_RAdam, RAdam)  \nLabel smoothing (0.5 - 0.7)<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n4. Submissions, Conversion and Ensemble\nWe rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.  \nAfter that, we aggregated all these models in the tf.Module class. Converting directly from Keras resulted in lower speed (don't know why).  \nWe calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.  \nEfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:  \nEfficientnet-B0, fold 0 BERT, full data train DeBERTa, full data train  \nInterestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n5. PS. Need BETTER TFlite DepthwiseConv2D\nDepthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good). We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:  \nGiven a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is Conv2D(32, 3, groups = 32), the other is DepthwiseConv2D(3). However, after converting these two to tflite, the running time of the Conv2D is 5.05ms, and the running time of DepthwiseConv2D is 3.70ms. More strangely, a full convolution Conv2D(32, 3, groups = 1) with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.  \nThen we rewrote the depthwise-conv like this:  \nThe running time of this is 1.24 ms.  \nIn summary, our version (1.24ms) > full Conv2D with larger FLOPs (2.09ms) > DepthwiseConv2D (3.70ms) > Conv2D(C, groups = C) (5.05ms).  \nHowever, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.  \nBy the way, EfficientNet with ONNX was ~5 times faster than TFLite.  \ngithub code<end_of_turn>\n<start_of_turn>model\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nYou are given a text containing summaries of different part of a document.\nCreate one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n\n**Chapter 1: Introduction**\n\n- The task is to classify images into different categories.\n- We use an approach similar to audio spectrogram classification.\n- We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n\n**Chapter 1: Data Preprocessing**\n\n* Extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points.\n* Applied various augmentations to the data.\n* Implemented standard normalization.\n* Filled in NaN values with zeros.\n* Interpolated the time axis to a size of 160 using 'nearest' interpolation.\n\n**Chapter 2: Feature Extraction**\n\n* Only 61 points were kept, including 40 lip points and 21 hand points.\n* For left and right hand, the one with less NaN was kept.\n* If right hand was kept, mirror it to left hand.\n\n**Chapter 3: Feature Engineering**\n\n* Hand-crafted features were also used, including motion, distances, and cosine of angles.\n* Motion features consist of future motion and history motion.\n* Full 210 pairwise distances among 21 hand points were included.\n* 15 angles of 5 fingers were included.\n\n**Chapter 4: Data Augmentation**\n\n* Sequences longer than 96 were interpolated to 96.\n* Sequences shorter than 96 were unchanged.\n\n**Chapter 1: Augmentation Techniques**\n\n* Augmentation is a technique used to increase the size and diversity of training data.\n* It can be used to improve the generalization performance of deep learning models.\n* There are two main types of augmentation: random and guided.\n\n**Chapter 2: Random Augmentation**\n\n* Random augmentation involves randomly modifying the input data.\n* It can be used to increase the size of the training dataset and to introduce noise into the data.\n* Some common random augmentation techniques include random affine, random interpolation, flip pose, finger tree rotate, and mixup.\n\n**Chapter 3: Guided Augmentation**\n\n* Guided augmentation involves using a set of pre-defined transformations to modify the input data.\n* It can be used to control the style and content of the training data.\n* Some common guided augmentation techniques include random cropping, random flipping, and random cropping.\n\n**Chapter 4: Time and Frequency Masking**\n\n* Time and frequency masking is a technique that involves randomly masking out parts of the input data.\n* This technique can be used to control the spatial and temporal dynamics of the training data.\n* Time masking involves randomly dropping out entire time steps, while frequency masking involves randomly dropping out entire frequency bins.\n\n**Chapter 1: Data Preparation**\n\n- Split the dataset into 8 folds.\n- Use a random split for training and the full dataset for validation.\n- Implement weighted CrossEntropyLoss for class imbalance.\n- Choose an EfficientNet model with 5 blocks.\n- Use Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.\n\n**Chapter 2: Model Training**\n\n- Train on one fold with a random split (8 folds in total).\n- Use Optuna to tune most parameters.\n- Implement knowledge distillation in the 3-layer model.\n\n**Chapter 3: Evaluation**\n\n- Evaluate the model on the validation set.\n- Tune the learning rate and dropout probability with Optuna.\n- Use EarlyStopping to prevent overfitting.\n\n**Chapter 1: Model Conversion and Ensemble**\n\n- Rewrote all models in Keras and transferred PyTorch weights to them.\n- Speed boost of 30% for transformer model.\n- Rewriting DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.\n\n**Chapter 2: Ensemble**\n\n- Calculated ensemble weights for models trained on fold 0 using the local fold 0 score.\n- Applied these weights to the full dataset models.\n- Ensemble included: EfficientNet-B0, fold 0 BERT, full data train DeBERTa, full data train.\n\n**Chapter 3: Results**\n\n- EfficientNet-B0 achieved a leaderboard score of approximately 0.8.\n- Transformers improved the score to 0.81.\n\n**Chapter 1: Introduction**\n\n* Depthwise convolution is a type of convolution that is used to extract features from images.\n* Traditional CNN and ViT models are both used for depthwise convolution, but they can be computationally expensive.\n* In this paper, we propose a new depthwise convolution model that is faster than existing models.\n\n**Chapter 2: Background**\n\n* Depthwise convolution is a type of convolution that is used to extract features from images.\n* Traditional CNN and ViT models are both used for depthwise convolution, but they can be computationally expensive.\n* One way to reduce the computational cost of depthwise convolution is to use a depthwise convolution kernel.\n* Another way to reduce the computational cost of depthwise convolution is to use a group convolution kernel.\n\n**Chapter 3: Proposed Depthwise Convolution Model**\n\n* We propose a new depthwise convolution model that is faster than existing models.\n* Our model uses a group convolution kernel that is applied to the input image in a parallel fashion.\n* This allows our model to be much faster than traditional depthwise convolution models.\n\n**Chapter 4: Experimental Results**\n\n* We compare our new model to existing depthwise convolution models on a variety of tasks.\n* Our model outperforms existing models on all of the tasks that we tested.\n* We also compare our model to an EfficientNet model with ONNX, which is a state-of-the-art model for image classification.\n* Our model is ~5 times faster than EfficientNet with ONNX.\n\n**Chapter 5: Conclusion**\n\n* Our new depthwise convolution model is faster than existing models on all of the tasks that we tested.\n* Our model is also more efficient than EfficientNet with ONNX.\n* We believe that our model has the potential to be a significant improvement for computer vision applications.<end_of_turn>\n<start_of_turn>model\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's see the **Refine method** in action!","metadata":{}},{"cell_type":"code","source":"# Refine strategy \n\n# Define prompt for the first summarization\nprompt_template = \"\"\"<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n{text}<end_of_turn>\n<start_of_turn>model\"\"\"\nprompt_init = PromptTemplate.from_template(prompt_template)\n\n# Define prompt for the refine phase, enhancing the previous summary with the new information\nrefine_template = \"\"\"<bos><start_of_turn>user\nProduce a final document divided in chapters and bullet points.\nYou are given a text containing an existing summary to a certain point:\n\n{existing_answer}\n\nYou can now refine it (if necessary) with more context below.\n\n{text}\n\nGiven the new context, refine the original summary.<end_of_turn>\n<start_of_turn>model\"\"\"\nprompt_refine = PromptTemplate.from_template(refine_template)\n\n\nchain = load_summarize_chain(langchain_hf, chain_type='refine',\n                             return_intermediate_steps=True,\n                             input_key='input_documents',\n                             output_key='output_text',\n                             question_prompt=prompt_init,\n                             refine_prompt=prompt_refine)\n\nout_summary = chain.invoke(splits, return_only_outputs=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:46:09.863378Z","iopub.execute_input":"2024-04-14T23:46:09.863648Z","iopub.status.idle":"2024-04-14T23:47:00.234236Z","shell.execute_reply.started":"2024-04-14T23:46:09.863625Z","shell.execute_reply":"2024-04-14T23:47:00.233174Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"display(Markdown(out_summary['output_text'].replace('#', '')))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:47:00.235604Z","iopub.execute_input":"2024-04-14T23:47:00.235928Z","iopub.status.idle":"2024-04-14T23:47:00.242196Z","shell.execute_reply.started":"2024-04-14T23:47:00.235900Z","shell.execute_reply":"2024-04-14T23:47:00.241316Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Chapter 1: Introduction**\n\nThe task is to classify images into different categories. We use an approach similar to audio spectrogram classification. We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n\n**Chapter 5: Refinement and Ensemble**\n\n- We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%.\n- We used ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.\n- EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81.\n- The final ensemble included:\n  - EfficientNet-B0, fold 0 BERT, full data train DeBERTa, full data train"},"metadata":{}}]},{"cell_type":"markdown","source":"We can still set `verbose=True` as before, or we can directly inspect the intermediate steps created (**expand the cell output**):","metadata":{}},{"cell_type":"code","source":"print(\"\\n###############################\\n\".join(out_summary[\"intermediate_steps\"]))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:47:00.243443Z","iopub.execute_input":"2024-04-14T23:47:00.243780Z","iopub.status.idle":"2024-04-14T23:47:00.254476Z","shell.execute_reply.started":"2024-04-14T23:47:00.243749Z","shell.execute_reply":"2024-04-14T23:47:00.253528Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"**Chapter 1: Introduction**\n\n- The task is to classify images into different categories.\n- We use an approach similar to audio spectrogram classification.\n- We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n###############################\n**Chapter 1: Introduction**\n\n- The task is to classify images into different categories.\n- We use an approach similar to audio spectrogram classification.\n- We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n###############################\n**Chapter 1: Introduction**\n\nThe task is to classify images into different categories. We use an approach similar to audio spectrogram classification. We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n###############################\n**Chapter 1: Introduction**\n\nThe task is to classify images into different categories. We use an approach similar to audio spectrogram classification. We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n###############################\n**Chapter 1: Introduction**\n\nThe task is to classify images into different categories. We use an approach similar to audio spectrogram classification. We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n\n**Chapter 5: Refinement and Ensemble**\n\n- We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%.\n- We used ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.\n- EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81.\n- The final ensemble included:\n  - EfficientNet-B0, fold 0 BERT, full data train DeBERTa, full data train\n###############################\n**Chapter 1: Introduction**\n\nThe task is to classify images into different categories. We use an approach similar to audio spectrogram classification. We use multiple models, including EfficientNet-B0 and DeBERTa.\n\n**Chapter 2: Model Architecture**\n\n- EfficientNet-B0 model with input size of 160x80.\n- Transformer models (BERT and DeBERTa) as helper models.\n- The final solution consists of one EfficientNet-B0 with an input size of 160x80.\n\n**Chapter 3: Training**\n\n- We use 8 randomly split folds for training.\n- A single fold model is trained on each fold.\n- We use a single EfficientNet-B0 model with an input size of 160x80.\n\n**Chapter 4: Evaluation**\n\n- We use a single fold for evaluation.\n- The model has a CV score of 0.898.\n- The model has a leaderboard score of ~0.8.\n\n**Chapter 5: Refinement and Ensemble**\n\n- We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%.\n- We used ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.\n- EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81.\n- The final ensemble included:\n  - EfficientNet-B0, fold 0 BERT, full data train DeBERTa, full data train\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see how the summary **progressively grows** as new context is being passed to the previous step.\n\nConsidering the current capabilities of Gemma and the length of the writeup, I would probably choose **the Stuffing strategy**. However, with some prompt tuning, the **other methods could be a viable approach.**\n\n<div class=\"alert alert-block alert-warning\">\nMapReduce and Refine <b>heavily depends on the prompts</b> you provide!. <b>Fine-tuning the prompts</b> is mandatory to improve the results.\n</div>\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br><br>\n    - Dividing in <b>chunk based on HTML sections seems</b> to be a reasonable approach to preserve the logic of the document.<br>\n    - Gemma is able to handle large context window, therefore <b>Stuffing summarization</b> seems the go-to strategy for Kaggle writeups. <br>\n    - <b>Refine method is a valid alternative</b> given the progressive growth of the final output\n</div>\n\n---\n\n## DIY: Run on your own write-up!\n\nUse this section to test what we have discussed so far on your own write-up or a selected write-up! <br>\nI've **prepared all the necessary functions** to run your experiments, you simply need to **decide the summarization strategy** (stuffing, map_reduce of refine) and **the chunking_strategy** (html, character or html_character)!\n\nHere's an example:","metadata":{}},{"cell_type":"code","source":"def check_inputs(summarization_strategy, chunking_strategy):\n    \"\"\"\n    Sanitizing user input. If summarization_strategy is \"stuffing\", it ignores the chunking_strategy parameter.\n    \"\"\"\n    if summarization_strategy not in ['stuffing', 'map_reduce', 'refine']:\n        raise ValueError(f'Wrong parameter \"summarization_strategy\": select either \"stuffing\", \"map_reduce\" or \"refine\". \"{summarization_strategy}\" was chosen instead.')\n    if chunking_strategy not in ['html', 'character', 'html_character'] and summarization_strategy!='stuffing':\n        raise ValueError(f'Wrong parameter \"chunking_strategy\": select either \"html\", \"character\" or \"html_character\". \"{chunking_strategy}\" was chosen instead.')\n    \ndef prepare_prompt(langchain_pipeline, writeup, verbose):\n    \"\"\"\n    Apply chat template to the prompt\n    \"\"\"\n    print('> Applying chat template to the prompt') if verbose else None\n    messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\\n\\n{}\".format(writeup)\n    }\n    ]\n\n    prompt = langchain_pipeline.hf_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return prompt\n\ndef mapreduce_strategy(langchain_pipeline, chunks):\n    \"\"\"\n    MapReduce strategy implementation using LangChain\n    \"\"\"\n    # Define prompt for summarization of each chunk\n    prompt_template = \"\"\"<bos><start_of_turn>user\n    Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n    {text}<end_of_turn>\n    <start_of_turn>model\"\"\"\n    prompt_init = PromptTemplate.from_template(prompt_template)\n\n    # Define prompt for final output, the summary of summaries\n    combine_template = \"\"\"<bos><start_of_turn>user\n    You are given a text containing summaries of different part of a document.\n    Create one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n\n    {text}<end_of_turn>\n    <start_of_turn>model\"\"\"\n    combine_prompt = PromptTemplate.from_template(combine_template)\n\n    # Create the chain of summarization, using map_reduce\n    chain = load_summarize_chain(langchain_pipeline,\n                                 chain_type='map_reduce', \n                                 map_prompt=prompt_init, \n                                 combine_prompt=combine_prompt)\n\n    # Run the chain on the chunks\n    out_summary = chain.invoke(chunks)\n    \n    return out_summary\n\ndef refine_strategy(langchain_pipeline, chunks):\n    \"\"\"\n    Refine strategy implementation using LangChain\n    \"\"\"\n    # Define prompt for the first summarization\n    prompt_template = \"\"\"<bos><start_of_turn>user\n    Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n    {text}<end_of_turn>\n    <start_of_turn>model\"\"\"\n    prompt_init = PromptTemplate.from_template(prompt_template)\n\n    # Define prompt for the refine phase, enhancing the previous summary with the new information\n    refine_template = \"\"\"<bos><start_of_turn>user\n    Your job is to produce a final document divided in chapters and bullet points.\n    You are given a text containing an existing summary to a certain point:\n\n    {existing_answer}\n\n    You can now refine it (if necessary) with more context below.\n\n    {text}\n\n    Given the new context, refine the original summary.<end_of_turn>\n    <start_of_turn>model\"\"\"\n    prompt_refine = PromptTemplate.from_template(refine_template)\n\n    chain = load_summarize_chain(langchain_pipeline, chain_type='refine',\n                                 return_intermediate_steps=True,\n                                 input_key='input_documents',\n                                 output_key='output_text',\n                                 question_prompt=prompt_init,\n                                 refine_prompt=prompt_refine)\n\n    out_summary = chain.invoke(chunks, return_only_outputs=True)\n    return out_summary\n    \n    \ndef prepare_chunks(text_to_split, chunking_strategy, verbose):\n    \"\"\"\n    Prepare chunks for MapReduce or Refine summarization strategies. Chunking can be at html level, character level of both\n    using the chunking_strategy parameter.\n    \"\"\"\n    print(f'> Preparing text chunking. Strategy: {chunking_strategy}') if verbose else None\n    output_chunks = text_to_split # To avoid local variable referenced before assignment\n    \n    if (chunking_strategy == 'html') or (chunking_strategy == 'html_character'):   \n        print(f'> Splitting at HTML level') if verbose else None\n        # Split on HTML headers\n        headers_to_split_on = [\n            (\"h1\", \"Header 1\"),\n            (\"h2\", \"Header 2\")\n        ]\n        # Split the real HTML writeup based on headers\n        text_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=False)\n        output_chunks = text_splitter.split_text(text_to_split)\n        \n        print(f\"\"\"\n        Length writup: {len(text_to_split)}\n        Number of html splits: {len(output_chunks)}\n        Length of each split: {[len(i.page_content) for i in output_chunks]}\n        \"\"\") if verbose else None\n\n        # Enrich the content with the metadata information\n        for i, text in enumerate(output_chunks):\n            # Join the metadata and the content together\n            final_content = '\\n'.join(text.metadata.values()) + '\\n' + text.page_content\n            # Replace the old content with the enriched one\n            text.page_content = final_content\n\n    if (chunking_strategy == 'character') or (chunking_strategy == 'html_character'):  \n        print(f'> Splitting at character level (newline char)') if verbose else None\n        text_splitter = CharacterTextSplitter(separator='\\n', chunk_size=2000, chunk_overlap=100)\n        if chunking_strategy == 'character':\n            # split_text doesn't automatically create Documents object in LangChain if we only want character splitting (and no html)\n            docs = text_splitter.create_documents([output_chunks])\n            output_chunks = text_splitter.split_documents(docs)\n        else:\n            # otherwise, htmlsplitter does create documents that we can further split \n            output_chunks = text_splitter.split_documents(output_chunks)\n        \n        print(f\"\"\"\n        Number of final splits: {len(output_chunks)}\n        Length of each final split: {[len(i.page_content) for i in output_chunks]}\n        \"\"\") if verbose else None\n    \n    return output_chunks\n    \ndef summarize(langchain_pipeline, writeup, summarization_strategy='stuffing', chunking_strategy='html', verbose=True):\n    \"\"\"\n    Summarize function that applies the summarization_strategy and chunking_strategy to a write-up.\n    \"\"\"\n    check_inputs(summarization_strategy, chunking_strategy)\n    print('> Begin summarization') if verbose else None\n    if summarization_strategy == 'stuffing':\n        print('> Summarization strategy: Stuffing. Ignoring chunking_strategy') if verbose else None\n        prompt = prepare_prompt(langchain_pipeline=langchain_pipeline, writeup=writeup, verbose=verbose)\n        print('> Invoking chain...') if verbose else None\n        output = langchain_pipeline.invoke(prompt)\n    elif summarization_strategy == 'map_reduce':\n        print('> Summarization strategy: MapReduce') if verbose else None\n        chunks = prepare_chunks(writeup, chunking_strategy, verbose)\n        print('> Invoking chain...') if verbose else None\n        output = mapreduce_strategy(langchain_pipeline=langchain_pipeline, chunks=chunks)['output_text'].replace('\\n\\n','\\n')\n    else:\n        print('> Summarization strategy: Refine') if verbose else None\n        chunks = prepare_chunks(writeup, chunking_strategy, verbose)\n        print('> Invoking chain...') if verbose else None\n        output = refine_strategy(langchain_pipeline=langchain_pipeline, chunks=chunks)['output_text']\n\n    print('\\n########## SUMMARY ##########\\n') if verbose else None    \n    return output","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-14T23:47:00.256033Z","iopub.execute_input":"2024-04-14T23:47:00.256433Z","iopub.status.idle":"2024-04-14T23:47:00.282038Z","shell.execute_reply.started":"2024-04-14T23:47:00.256399Z","shell.execute_reply":"2024-04-14T23:47:00.281296Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Example using a random writeup\n\ncustom_writeup = writeups.iloc[20, 9]\n\nprint(custom_writeup[:500])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:47:00.283095Z","iopub.execute_input":"2024-04-14T23:47:00.283401Z","iopub.status.idle":"2024-04-14T23:47:00.298830Z","shell.execute_reply.started":"2024-04-14T23:47:00.283378Z","shell.execute_reply":"2024-04-14T23:47:00.297847Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"<p>Team members are junseonglee11 (@junseonglee11), Ayaan Jang(@ayaanjang). <br>\nWe ensembled 6 LSTM models (2 different versions).<br>\nWe modified Robin Smith's and Robert Hatch's notebooks.  </p>\n<h1><strong>Our notebooks:</strong></h1>\n<p>Inference: <a href=\"https://www.kaggle.com/code/ayaanjang/20th-tensorflow-lstm-model-inference-merged\" target=\"_blank\">https://www.kaggle.com/code/ayaanjang/20th-tensorflow-lstm-model-inference-merged</a><br>\nTrain: <a href=\"https://www.kaggle.com/code/junse\n","output_type":"stream"}]},{"cell_type":"code","source":"# How to call the function\noutput = summarize(\n    langchain_pipeline=langchain_hf, # Select the LangChain wrapper around the HF pipeline we init before\n    writeup=custom_writeup, # Pass the writeup\n    summarization_strategy='stuffing', # Select the summarization strategy\n    chunking_strategy='html' # Select the chunking strategy. If stuffing, this parameter is ignored\n)\ndisplay(Markdown(output.replace('#', ''))) # Replace possible headings to avoid conflict with Kaggle ToC","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:47:00.300429Z","iopub.execute_input":"2024-04-14T23:47:00.300796Z","iopub.status.idle":"2024-04-14T23:47:13.667290Z","shell.execute_reply.started":"2024-04-14T23:47:00.300764Z","shell.execute_reply":"2024-04-14T23:47:13.666247Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"> Begin summarization\n> Summarization strategy: Stuffing. Ignoring chunking_strategy\n> Applying chat template to the prompt\n> Invoking chain...\n\n########## SUMMARY ##########\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" Summary\n\n**Chapter 1: Introduction**\n\n* Team members used 6 LSTM models for an RNN model.\n* They modified existing notebooks and created new ones for data preprocessing and training.\n\n**Chapter 2: Data and Preprocessing**\n\n* They preprocessed 96 time-series features from the \"icecube\" dataset.\n* They experimented with different feature engineering techniques to improve prediction accuracy.\n* They converted features to TFRecord format for efficient training.\n\n**Chapter 3: Model Training and Inference**\n\n* They trained two versions of the model (with and without feature square root).\n* They used Adam optimizer and ensemble techniques to combine models.\n* They split the dataset into 10 folds for training and validation.\n\n**Chapter 4: Data Postprocessing**\n\n* They modified the code to perform weighted average of predicted probabilities and calculate the azimuth and zenith directions of the particle.\n* They experimented with different post-processing techniques to improve the model's performance.\n\n**Chapter 5: Results and Discussion**\n\n* They presented the results of training and post-processing.\n* They discussed the importance of feature engineering and ensemble techniques for improving the model's accuracy."},"metadata":{}}]},{"cell_type":"code","source":"# Another example using a different strategy\noutput = summarize(\n    langchain_pipeline=langchain_hf, \n    writeup=custom_writeup, \n    summarization_strategy='refine', \n    chunking_strategy='html')\n\ndisplay(Markdown(output.replace('#', '')))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:47:13.668832Z","iopub.execute_input":"2024-04-14T23:47:13.669240Z","iopub.status.idle":"2024-04-14T23:48:06.094944Z","shell.execute_reply.started":"2024-04-14T23:47:13.669205Z","shell.execute_reply":"2024-04-14T23:48:06.093974Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"> Begin summarization\n> Summarization strategy: Refine\n> Preparing text chunking. Strategy: html\n> Splitting at HTML level\n\n        Length writup: 7083\n        Number of html splits: 6\n        Length of each split: [179, 282, 586, 1818, 848, 1000]\n        \n> Invoking chain...\n\n########## SUMMARY ##########\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Chapter 1: Introduction**\n\n**LSTM Models**\n\nLSTM models are a powerful type of recurrent neural network (RNN) known for their ability to process sequential data effectively. They possess a unique structure with feedback loops that enable them to capture long-term dependencies between consecutive data points.\n\n**Chapter 2: Ensemble Learning**\n\n**Model Selection and Configuration**\n\nWe select two distinct versions of LSTM models:\n\n* Robin Smith's notebook\n* Robert Hatch's notebook\n\nWe optimize these notebooks by adjusting hyperparameters and experimenting with different configurations.\n\n**Chapter 3: Experimentation and Evaluation**\n\nWe conduct a comprehensive set of experiments to identify the optimal settings for the ensemble. We employ various metrics to evaluate the performance of the ensemble, including accuracy, precision, and recall.\n\n**Chapter 4: Results and Discussion**\n\nThe results demonstrate that the ensemble of 2 LSTM models achieves a significant improvement in performance compared to the individual models. We analyze the insights gained from the analysis to understand the strengths and weaknesses of each model.\n\n**Chapter 5: Conclusion**\n\nIn this project, we explored ensemble learning for LSTM models, showcasing its effectiveness in enhancing performance. The results provide valuable insights into the power of combining multiple models for improved accuracy and robustness."},"metadata":{}}]},{"cell_type":"markdown","source":"---\n\n# Fine-tuning Gemma with LoRa\n\nEarlier we saw that we can achieve **great results by optimizing our prompts**. However, there might be times where [fine-tuning a model would work better](https://huggingface.co/docs/transformers/tasks/prompting#prompting-vs-fine-tuning):\n\n- The domain is wildly different from what LLMs were pre-trained on and prompt optimization did not yield sufficient results.\n- We need our model to work well in a low-resource language.\n- We need the model to be trained on sensitive data that is under strict regulations.\n- We have to use a small model due to cost, privacy, infrastructure or other limitations.\n\nIn such scenarios, we will need to fine-tuned our model on a **domain-specific dataset**. <br> \n\nFine-tuning a LLM is **computational demanding**. In order to **train it on commercial laptops**, the only feasible way to save memory is by **reducing the model size.** Building on this, the idea is to **fine-tune only few parameters** rather than the entire model, a method currently used extensively called **Parameter Efficient Fine-Tuning or PEFT.**\nFor more details about PEFT and, specifically, **LoRa**, the technique we are going to use now, I encourage you to read this beautiful [blog post](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-tune-an-LLM-Part-3-The-HuggingFace-Trainer--Vmlldzo1OTEyNjMy#parameter-efficient-fine-tuning-(peft)).\n\nLet's now see how **Gemma can be fine-tuned** using HuggingFace [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) and [datasets](https://huggingface.co/docs/datasets/index). \n\nTo do so, I'll use the [CNN daily news dataset](https://huggingface.co/datasets/cnn_dailymail#dataset-card-for-cnn-dailymail-dataset), also available on Kaggle. This dataset comprises **news articles written by journalists at CNN and the Daily Mail**. For each instance, there is a string for the article and a **string for the highlights (summary).**\n\n<div class=\"alert alert-block alert-warning\">\nCurrently, there are <b> no domain-specific dataset </b> to fine-tune our models on <b>Kaggle writeups </b>. <br> We will utilize the CNN news dataset simply to <b>demonstrate the benefits of fine-tuning which could potentially apply to our specific use case as well</b>.\n</div>","metadata":{}},{"cell_type":"code","source":"# Make space in memory\nlangchain_hf = release_memory(langchain_hf)\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:48:06.096134Z","iopub.execute_input":"2024-04-14T23:48:06.096469Z","iopub.status.idle":"2024-04-14T23:48:06.861131Z","shell.execute_reply.started":"2024-04-14T23:48:06.096443Z","shell.execute_reply":"2024-04-14T23:48:06.859990Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Import of the validation set which contains fewer examples than training\nvalidation = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')[['article', 'highlights']]\nvalidation.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:48:06.862523Z","iopub.execute_input":"2024-04-14T23:48:06.862949Z","iopub.status.idle":"2024-04-14T23:48:08.123560Z","shell.execute_reply.started":"2024-04-14T23:48:06.862910Z","shell.execute_reply":"2024-04-14T23:48:08.122616Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                             article  \\\n0  Sally Forrest, an actress-dancer who graced th...   \n1  A middle-school teacher in China has inked hun...   \n2  A man convicted of killing the father and sist...   \n3  Avid rugby fan Prince Harry could barely watch...   \n4  A Triple M Radio producer has been inundated w...   \n\n                                          highlights  \n0  Sally Forrest, an actress-dancer who graced th...  \n1  Works include pictures of Presidential Palace ...  \n2  Iftekhar Murtaza, 29, was convicted a year ago...  \n3  Prince Harry in attendance for England's crunc...  \n4  Nick Slater's colleagues uploaded a picture to...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sally Forrest, an actress-dancer who graced th...</td>\n      <td>Sally Forrest, an actress-dancer who graced th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A middle-school teacher in China has inked hun...</td>\n      <td>Works include pictures of Presidential Palace ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A man convicted of killing the father and sist...</td>\n      <td>Iftekhar Murtaza, 29, was convicted a year ago...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Avid rugby fan Prince Harry could barely watch...</td>\n      <td>Prince Harry in attendance for England's crunc...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Triple M Radio producer has been inundated w...</td>\n      <td>Nick Slater's colleagues uploaded a picture to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's now setup the LoRa [configurations](https://huggingface.co/blog/gemma-peft#low-rank-adaptation-for-large-language-models), mainly following this [blog post](https://huggingface.co/blog/gemma-peft#low-rank-adaptation-for-large-language-models) from HuggingFace.","metadata":{}},{"cell_type":"code","source":"model = \"/kaggle/input/gemma/transformers/2b-it/3\"\n\nlora_config = LoraConfig(\n    r=6,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\ntokenizer.padding_side = \"right\" # Fixing overflow issue ref: source code\nmodel = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=bnb_config)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:48:08.124719Z","iopub.execute_input":"2024-04-14T23:48:08.125014Z","iopub.status.idle":"2024-04-14T23:48:13.775747Z","shell.execute_reply.started":"2024-04-14T23:48:08.124989Z","shell.execute_reply":"2024-04-14T23:48:13.774986Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e802e0698564f918f30d68f2141a5ef"}},"metadata":{}}]},{"cell_type":"markdown","source":"Once again, we need to pay attention on how we are going to [format our examples](https://huggingface.co/docs/trl/sft_trainer#dataset-format-support) (remember the chat template?). This time, we make sure we are not generating the model response at the end of the string by setting `add_generation_prompt = False`:","metadata":{}},{"cell_type":"code","source":"train_data = Dataset.from_pandas(validation)\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['article'])):\n        messages = [\n            {\"role\": \"user\",\n             \"content\": \"Given the following article, write a short summary of the article in 2-3 sentences:\\n\\nArticle: {}\".format(example['article'][i])},\n            {\"role\": \"assistant\",\n             \"content\": \"{}\".format(example['highlights'][i])}\n        ]\n        output_texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))\n        \n    return output_texts\n\n# Print the first training example\nprint(formatting_prompts_func(train_data[:1])[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:48:13.776964Z","iopub.execute_input":"2024-04-14T23:48:13.777278Z","iopub.status.idle":"2024-04-14T23:48:14.259535Z","shell.execute_reply.started":"2024-04-14T23:48:13.777252Z","shell.execute_reply":"2024-04-14T23:48:14.258538Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nGiven the following article, write a short summary of the article in 2-3 sentences:\n\nArticle: Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .<end_of_turn>\n<start_of_turn>model\nSally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .<end_of_turn>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can launch the actual training phase:","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    max_seq_length=512,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=12,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        report_to='none',\n        output_dir='logs',\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_prompts_func,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:48:14.260759Z","iopub.execute_input":"2024-04-14T23:48:14.261041Z","iopub.status.idle":"2024-04-14T23:49:31.955538Z","shell.execute_reply.started":"2024-04-14T23:48:14.261018Z","shell.execute_reply":"2024-04-14T23:49:31.954623Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3d8e337e4745c9a29ff5a892fc6eed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:49, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.514400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.831700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.750300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.710400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.267800</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.316900</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.217000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.276300</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.719500</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.890400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.966300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.616700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12, training_loss=3.256467600663503, metrics={'train_runtime': 54.5748, 'train_samples_per_second': 0.88, 'train_steps_per_second': 0.22, 'total_flos': 286748770713600.0, 'train_loss': 3.256467600663503, 'epoch': 0.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained('lora_adapter')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:49:31.956707Z","iopub.execute_input":"2024-04-14T23:49:31.957001Z","iopub.status.idle":"2024-04-14T23:49:32.058519Z","shell.execute_reply.started":"2024-04-14T23:49:31.956976Z","shell.execute_reply":"2024-04-14T23:49:32.057680Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"We now have saved the LoRa adapter which can be later be used to either load the model or continue training if necessary.","metadata":{}},{"cell_type":"code","source":"# Make space in memory\ntrainer, model, tokenizer = release_memory(trainer, model, tokenizer)\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-14T23:49:32.059841Z","iopub.execute_input":"2024-04-14T23:49:32.060152Z","iopub.status.idle":"2024-04-14T23:49:32.999549Z","shell.execute_reply.started":"2024-04-14T23:49:32.060126Z","shell.execute_reply":"2024-04-14T23:49:32.998541Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"We could continue with our model, but for the sake of this tutorial I'll show how to [load our adapter and merge into Gemma pretrained model](https://huggingface.co/docs/trl/use_model#use-adapters-peft).","metadata":{}},{"cell_type":"code","source":"# Load the pretrained model and our LoRa adapter\nbase_model_name = \"/kaggle/input/gemma/transformers/2b-it/3\"\nadapter_model_name = \"/kaggle/working/lora_adapter\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, device_map='auto', torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model, adapter_model_name, device_map='auto', torch_dtype=torch.float16)\n\n# Merge the adapters into the base model so you can use the model like a normal transformers model\nmodel = model.merge_and_unload()\nmodel.save_pretrained('final_model')\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:49:33.001041Z","iopub.execute_input":"2024-04-14T23:49:33.001455Z","iopub.status.idle":"2024-04-14T23:50:07.511051Z","shell.execute_reply.started":"2024-04-14T23:49:33.001421Z","shell.execute_reply":"2024-04-14T23:50:07.510199Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a5ac07ded3464698795d305c6acf18"}},"metadata":{}}]},{"cell_type":"code","source":"model = \"/kaggle/working/final_model\"\n\n# Load the HF pipeline using our newly fine-tuned Gemma 2B\npipe_finetuned = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    model_kwargs={\"torch_dtype\": torch.float16},\n    device_map='auto',\n    max_new_tokens=512\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-14T23:50:07.512498Z","iopub.execute_input":"2024-04-14T23:50:07.512958Z","iopub.status.idle":"2024-04-14T23:50:09.966416Z","shell.execute_reply.started":"2024-04-14T23:50:07.512924Z","shell.execute_reply":"2024-04-14T23:50:09.965407Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc7564ff46e04495b59be2abc5abad1f"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's see it in action on the same writeup we analyzed before:","metadata":{}},{"cell_type":"code","source":"outputs = pipe_finetuned(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\n\ndisplay(Markdown(outputs[0][\"generated_text\"][len(prompt):].replace('#', '')))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:50:09.967694Z","iopub.execute_input":"2024-04-14T23:50:09.968006Z","iopub.status.idle":"2024-04-14T23:50:25.041610Z","shell.execute_reply.started":"2024-04-14T23:50:09.967981Z","shell.execute_reply":"2024-04-14T23:50:25.040319Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" Summary of the text:\n\n**Data Preprocessing:**\n\n* Extract 80 features from each image (160x80).\n* Apply various augmentations and normalization techniques.\n* Use a mix of common and CNN specific augmentations.\n\n**Training:**\n\n* Use EfficientNet-B0 for CNN training.\n* Use a transformer model for the transformer part.\n* Tune hyperparameters with Optuna.\n* Aggregate models in a tf.Module for ensemble.\n\n**Submission:**\n\n* Rewrote the DepthwiseConv2D operation for faster execution.\n* Ensemble multiple models for improved performance.\n\n**Key Takeaways:**\n\n* Ensemble of EfficientNet-B0, BERT, and DeBERTa achieved the highest leaderboard score of 0.81.\n* EfficientNet-B0 achieved a leaderboard score of 0.8.\n* DepthwiseConv2D outperformed other CNN and ViT models.\n* Ensemble of CNN models achieved a leaderboard score of 0.82."},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see, our fine-tuned model is **still able to follow our instructions**. But how can we assess if the extra training worked? \n\nGiven that we **requested concise summaries** during the fine-tuning process, let's **compare** our base model with the fine-tuned model on **this specific task**:","metadata":{}},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Write a short summary of 2-3 sentences of the following text:\\n\\n{}\".format(writeup)\n    }\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Using the previous pipeline with the previous model\noutputs = pipe(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\n\ndisplay(Markdown(outputs[0][\"generated_text\"][len(prompt):].replace('#', ''))) # Replace headings to avoid conflict with Kaggle ToC","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:50:25.043548Z","iopub.execute_input":"2024-04-14T23:50:25.044433Z","iopub.status.idle":"2024-04-14T23:50:36.266201Z","shell.execute_reply.started":"2024-04-14T23:50:25.044398Z","shell.execute_reply":"2024-04-14T23:50:36.265181Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Sure, here's a summary of the text:\n\n- The paper proposes an ensemble of EfficientNet-B0, BERT, and DeBERTa models for lip and pose recognition.\n- The ensemble is trained on a single fold with a random split and then converted to TFlite format.\n- The EfficientNet-B0 model achieves a leaderboard score of 0.8, while the BERT and DeBERTa models achieve scores of 0.81 and 0.80, respectively.\n- The ensemble outperforms the individual models, with the EfficientNet-B0 model achieving the highest accuracy.\n- The paper also provides insights into the differences between different depthwise convolution implementations in TFlite."},"metadata":{}}]},{"cell_type":"code","source":"# Using the pipeline with the fine-tuned model\noutputs = pipe_finetuned(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\n\ndisplay(Markdown(outputs[0][\"generated_text\"][len(prompt):].replace('#', '')))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:50:36.267516Z","iopub.execute_input":"2024-04-14T23:50:36.267807Z","iopub.status.idle":"2024-04-14T23:50:46.802855Z","shell.execute_reply.started":"2024-04-14T23:50:36.267782Z","shell.execute_reply":"2024-04-14T23:50:46.801744Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Sure, here's a summary of the text:\n\nThe text describes the training of an EfficientNet-B0 model and a BERT model on a dataset of lip and pose data. The EfficientNet-B0 model achieved a leaderboard score of 0.8, while the BERT model achieved a score of 0.81. The ensemble of these models achieved a score of 0.82.\n\nThe text also provides details about the data preprocessing, augmentation, and training process. It also discusses the differences between the EfficientNet-B0 and BERT models, and the reasons why the ensemble of these models achieved a higher score than either model alone."},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that **fine-tuning improved our output**: the model now **adheres better to our instructions** and summarize **the writeup in few sentences**.\n\n> Looking ahead, it would be beneficial for the Kaggle community to develop a **domain-specific dataset** for fine-tuning Gemma on Kaggle write-ups. The curated summaries could adhere to a defined format, such as highlighting key aspects and identifying any omissions in the [Kaggle template](https://www.kaggle.com/solution-write-up-documentation). <br>\n\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br><br>\n    - <b>Fine-tuning</b> proves beneficial in cases where <b>optimizing the prompt does not yield satisfactory results</b>. In these instances, a <b>domain-specific dataset</b> can greatly improve the desidered output.<br>\n    - Default (full weight) LLMs are <b>memory and compute-intensive</b>, which may render fine-tuning impractical. Parameter Efficient Fine-Tuning or <b>PEFT</b> is a potent approach that enables the model to be \"downsized,\" achieving <b>performance comparable to full fine-tuning with only a limited number of trainable parameters.</b> <br>\n</div>\n\n---\n\n# Conclusions and next steps\n\nIn this notebook, we explore the intricate world of text summarization using **Gemma capabilities** together with **HuggingFace and LangChain abstractions.** <br>\nWe progressed from merely requesting a summary to understard the **essential setup and parameters** required for meaningful results. This includes prompt engineering, chat template, chunking and summarization strategies and fine-tuning. <br>\n\n>Why is this useful? <br>\nFrom a Kaggle perspective, I foresee the possibility of having a personalized assistant on the website, helping users navigate through Kaggle complexity both on forum and on competition cards.\n\nThere are still many aspects worth exploring:\n- Text preprocessing to clean and prepare Kaggle writeups\n- Optimizing chunking parameters such as `chunk_size`, `chunk_overlap` and headers/characters to split on\n- Fine-tuning MapReduce and Refine prompts to improve results\n- Fine-tune Gemma on domain-specific dataset (when available) \n- Assess summaries quality quantitatively\n\nThe last point is interesting and one approach could be to implement the findings outlined in the [QAGS paper](https://aclanthology.org/2020.acl-main.450.pdf). This poses an intriguing question: **how can we ensure we obtained factual consistency in our summary?**\n\nThe paper talks about a framework called **QAGS (Question Answering and Generation for Summarization)** and it can be summarized with one image:\n\n<img src=\"https://i.imgur.com/NX6P37e.png\" width=\"600\">\n\nAn idea could be to implement the following pipeline:\n- Use a model to generate Q&A on the summary produced by Gemma\n- Collect answers for both the summary and the input text by querying a model\n- Examine consistency and identify hallucinations, that is if facts are present only in the summary but not in the original text.\n\n**Stay tuned, any feedback will be appreciated!**","metadata":{}}]}